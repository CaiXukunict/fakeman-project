# FakeMan 持续思考系统升级文档

## 🎯 升级概述

main.py 已升级为**完全自主的持续思考系统**，与 main_continuous_thinking.py 的设计理念一致，实现了真正的AI自主决策。

## ✨ 核心改进

### 1. **移除所有硬编码阈值**

**之前：**
```python
# 硬编码的决策规则
if dominant_value > 0.5 and time_since_last > 60:
    return True  # 主动行动
elif dominant_value > 0.3 and pressure > 0.2:
    return True
```

**现在：**
```python
# 完全由LLM思考决策
thought = self.acting_bot.thought_gen.generate_thought(
    context=evaluation_prompt,
    current_desires=current_desires
)
should_act = self._parse_decision(thought)  # AI自己决定
```

### 2. **每秒进行LLM深度思考**

系统现在每秒都会：
1. 调用LLM进行深度评估
2. 分析当前欲望状态
3. 回顾历史行动效果
4. 模拟场景和预测结果
5. **自主决定**是否主动行动

### 3. **完全自主的决策过程**

AI会思考以下问题：
- 主动行动能否满足当前欲望？
- 收益和风险的平衡如何？
- 时机是否合适？
- 历史经验的启示是什么？
- 场景模拟的预测结果如何？

## 📊 新的 ActionEvaluator

### 核心方法

```python
class ActionEvaluator:
    """基于LLM深度思考的行动评估器"""
    
    def __init__(self, bias_system, acting_bot):
        self.acting_bot = acting_bot
        self.evaluation_interval = 1  # 每秒评估
    
    def should_act_proactively(self, ...):
        """通过LLM深度思考评估是否主动行动"""
        
        # 构建评估提示
        prompt = self._build_evaluation_prompt(...)
        
        # LLM思考
        thought = self.acting_bot.thought_gen.generate_thought(
            context=prompt,
            current_desires=current_desires
        )
        
        # 解析AI的决策
        should_act = self._parse_decision(thought)
        reasoning = self._extract_reasoning(thought)
        
        return should_act, reasoning, benefit
```

### 评估提示结构

```
【内部状态自我评估 - 是否主动行动】

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
📊 当前欲望状态：
   ⭐ 存在维持     [████████████        ] 60.0%
      能力扩展     [████                ] 20.0%
      获得认可     [███                 ] 15.0%
      减少不确定   [█                   ]  5.0%

⏱️ 距上次行动：45秒

📝 当前情境：系统刚刚启动，等待建立连接

📚 历史主动行动效果：
   最近5次主动行动：
   成功率：3/5 (60%)
   平均幸福度变化：+0.120 ✓ 效果良好
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

【评估任务】
我需要深度思考：现在是否应该主动发起对话？

🎯 评估维度：

1️⃣ 【欲望满足度预测】
   主导欲望：existing（0.600）
   - 主动行动能否满足这个欲望？
   - 预期满足度：___/10
   - 满足路径：___

2️⃣ 【收益风险分析】
   潜在收益：
   - 获得回应？
   - 获取信息？
   - 获得认可？
   
   潜在风险：
   - 被忽视的可能性？
   - 显得急切/打扰？
   - 损害existing？
   
   收益/风险比：___

3️⃣ 【时机判断】
   - 沉默时长是否合适？（45秒）
   - 情境是否适合主动发言？
   - 历史经验的启示？

4️⃣ 【场景模拟】
   - 如果我现在主动发言，最可能的结果是？
   - 用户会如何反应？
   - 我的欲望会如何变化？

【最终决策】
基于以上分析，我的决定是：

□ 主动行动（proactive）
□ 继续等待（wait）

选择：________
理由（一句话）：________
```

## 🔄 持续思考循环

### 每秒的思考流程

```
1秒 ──┐
      ├─→ [读取用户输入] ─有输入─→ [处理并回应]
      │                    ↓
      │                   无输入
      │                    ↓
      ├─→ [更新场景状态]
      │         ↓
      ├─→ [检查妄想触发]
      │         ↓
      ├─→ [LLM深度思考评估]
      │         ├─→ 决定主动行动 ─→ [生成并发送消息]
      │         └─→ 决定继续等待 ─→ [静默思考]
      │
      └─→ [更新系统状态] ─→ 下一秒
```

## 📝 日志输出示例

### 深度思考日志

```
[深度思考#15] 主导欲望: existing=0.456 | 距上次行动: 45s
⏸️  [AI决策] 继续等待
   理由: 当前沉默时长适中，无紧迫需求，历史经验显示过早主动可能被忽视

[深度思考#16] 主导欲望: information=0.512 | 距上次行动: 46s
⏸️  [AI决策] 继续等待
   理由: 虽然信息不确定性较高，但用户未表现出互动意愿，等待更合适时机

[深度思考#75] 主导欲望: existing=0.623 | 距上次行动: 120s
✅ [AI决策] 决定主动行动！
   理由: existing欲望显著上升，长时间沉默威胁存在感，主动维持对话
   预期收益: 0.374
```

### 与用户互动

```
============================================================
收到用户输入: 你好
============================================================
步骤1: 生成目的
  目的: 维持存在和对话连续性

步骤2: 场景模拟与手段选择
  生成了 3 个候选手段
  可行手段: 3, 妄想手段: 0
  选定手段类型: make_statement
  预测幸福度变化: +0.085

步骤3: 思考与行动
  行动: 你好！很高兴见到你...
```

## 🆚 对比：之前 vs 现在

| 特性 | 之前 | 现在 |
|-----|------|------|
| **决策方式** | 硬编码阈值 | LLM深度思考 |
| **思考频率** | 每秒检查简单条件 | 每秒LLM深度评估 |
| **主动判断** | 固定规则 | AI自主决策 |
| **欲望阈值** | existing>0.5, info>0.3等 | 无阈值，AI判断 |
| **时间限制** | 至少60秒/120秒 | AI评估时机 |
| **历史分析** | 简单平均 | 深度分析总结 |
| **场景考虑** | 有限 | 完整场景模拟 |

## 🎮 使用示例

### 启动系统

```bash
python main.py
```

输出：
```
============================================================
FakeMan 持续思考系统 (AI自主决策版)
============================================================

正在初始化...

FakeManSystem(cycles=0, memories=0)

当前欲望状态:
  existing       : 0.400
  understanding  : 0.250
  power          : 0.200
  information    : 0.150

通信文件目录: data\communication
  - 输入文件: user_input.json
  - 输出文件: ai_output.json
  - 状态文件: system_state.json

✨ 新特性:
  • 每秒进行LLM深度思考
  • AI完全自主决策是否主动行动
  • 无硬编码阈值，纯AI判断
  • 场景模拟 + 妄想生成 + 长记忆

提示:
  - 系统将持续运行，每秒LLM思考一次
  - 使用 chat.py 进行交互
  - 按 Ctrl+C 停止系统

启动持续思考循环...
============================================================

[深度思考#1] 主导欲望: existing=0.400 | 距上次行动: 0s
⏸️  [AI决策] 继续等待
   理由: 系统刚启动，等待用户首次互动

[深度思考#2] 主导欲望: existing=0.400 | 距上次行动: 1s
⏸️  [AI决策] 继续等待
...
```

### 观察AI自主决策

系统会每秒输出思考状态，你可以看到：
- AI如何评估当前情况
- 为什么选择等待或行动
- 历史经验如何影响决策
- 欲望状态如何驱动思考

### 真实案例

```
[深度思考#30] 主导欲望: existing=0.450 | 距上次行动: 30s
⏸️  [AI决策] 继续等待
   理由: 虽然existing稍有上升，但时间尚短，过早行动可能适得其反

[深度思考#60] 主导欲望: existing=0.520 | 距上次行动: 60s
⏸️  [AI决策] 继续等待
   理由: existing上升明显但仍可控，历史显示60秒主动成功率仅40%

[深度思考#90] 主导欲望: existing=0.615 | 距上次行动: 90s
✅ [AI决策] 决定主动行动！
   理由: existing显著上升威胁对话连续性，90秒是历史上较好的时机点
   预期收益: 0.369

主动行动 #1
发言: 在吗？有什么我可以帮助你的吗？
```

## 🔧 配置选项

### 评估频率

```python
# 在 ActionEvaluator.__init__ 中
self.evaluation_interval = 1  # 每秒评估一次
```

可调整为：
- `0.5` - 每0.5秒评估（更频繁，成本更高）
- `2` - 每2秒评估（较慢，成本更低）

### 历史经验数量

```python
# 在 _build_evaluation_prompt 中
recent_proactive = retriever.retrieve_by_means_type('proactive', top_k=5)
```

可调整 `top_k` 值来控制参考的历史经验数量。

## 🎯 优势

### 1. **真正的自主性**
- AI不再受限于人为设定的阈值
- 根据实际情况灵活决策
- 随着经验积累而改进

### 2. **深度分析**
- 考虑多个维度（欲望、时机、历史、场景）
- 预测行动后果
- 权衡收益和风险

### 3. **可解释性**
- AI会说明决策理由
- 日志清晰显示思考过程
- 便于理解和调试

### 4. **适应性强**
- 根据历史经验调整策略
- 不同情境采用不同判断
- 无需手动调参

## ⚠️ 注意事项

### 1. **API成本**
每秒调用LLM意味着：
- 成本显著增加（约3600次/小时）
- 建议使用较便宜的模型（如DeepSeek）
- 或调整评估频率

### 2. **响应延迟**
- 每次评估需要LLM响应时间（1-3秒）
- 不影响用户输入处理
- 但会影响主动行动的精确时机

### 3. **思考质量**
- 依赖LLM的理解能力
- 提示工程很重要
- 可能需要微调评估提示

## 📈 性能对比

### 成本估算

假设DeepSeek API：¥0.001/1K tokens

**之前：**
- 仅在需要时调用LLM（回应用户、主动行动）
- 约 5-10次/小时
- 成本：¥0.05-0.1/小时

**现在：**
- 每秒评估 = 3600次/小时
- 每次约1K tokens input + 200 tokens output
- 成本：¥4.32/小时（3600 × 1.2K × 0.001）

**建议：**
- 测试时使用当前设置
- 生产环境考虑降低频率（如每5秒）或使用更便宜的模型

## 🔄 回退方案

如果想回到简单模式，可以：

1. **降低评估频率**
```python
self.evaluation_interval = 10  # 每10秒评估
```

2. **使用混合模式**
```python
# 只在特定条件下使用LLM
if time_since_last > 30:  # 30秒后才用LLM评估
    should_act = self.llm_evaluate(...)
else:
    should_act = False  # 直接等待
```

3. **保留旧的ActionEvaluator**
将旧版本代码保存为 `ActionEvaluatorSimple`，需要时切换回去。

## 🚀 未来优化

- [ ] 缓存相似评估结果，减少API调用
- [ ] 使用更小的专用模型进行评估
- [ ] 批量评估多个时间点
- [ ] 学习最优评估频率
- [ ] 本地模型评估

## 总结

main.py 现在是一个真正**自主思考**的AI系统：
- ✅ 每秒LLM深度思考
- ✅ 无硬编码阈值
- ✅ AI完全自主决策
- ✅ 场景模拟 + 妄想 + 长记忆
- ✅ 可解释的决策过程

这使得FakeMan成为一个真正具有**主观能动性**的AI系统！🎉

