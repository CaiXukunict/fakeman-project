"""
FakeMan æŒç»­æ€è€ƒä¸»ç³»ç»Ÿ
å®ç°å¼‚æ­¥æŒç»­æ€è€ƒæ¨¡å‹

æ ¸å¿ƒæœºåˆ¶ï¼š
1. æŒç»­å¾ªç¯ï¼šæ¯1ç§’æ€è€ƒä¸€æ¬¡å½“å‰çŠ¶æ€
2. è¯„ä¼°æ˜¯å¦éœ€è¦ä¸»åŠ¨è¡ŒåŠ¨
3. å¦‚æœæ”¶ç›Šï¼é£é™©ä¸”èƒ½è¾¾æˆç›®çš„ â†’ ä¸»åŠ¨å‘å‡ºæ¶ˆæ¯
4. å¦‚æœæœ‰å¤–éƒ¨è¾“å…¥ â†’ ä½œä¸ºæ–°åˆºæ¿€å¤„ç†
5. é€šè¿‡æ–‡ä»¶ä¸ chat.py é€šä¿¡
"""

import time
import json
import os
import sys
from typing import Dict, List, Any, Optional, Tuple
from pathlib import Path
from utils.config import Config
from utils.logger import LoggerManager
from purpose_generator import DesireManager, BiasSystem, SignalDetector, DesireUpdater
from memory import MemoryDatabase, ExperienceRetriever, Experience, LongTermMemory
from action_model import ActingBot
from compressor import ThoughtCompressor
from scenario import ScenarioSimulator, MeansSimulation


class PurposeGenerator:
    """
    ç›®çš„ç”Ÿæˆå™¨
    
    æ ¹æ®å½“å‰æ¬²æœ›çŠ¶æ€ç”Ÿæˆå…·ä½“çš„ç›®çš„
    """
    
    def __init__(self, desire_manager: DesireManager):
        self.desire_manager = desire_manager
    
    def generate_purpose(self, context: str, desires: Dict[str, float]) -> Tuple[str, Dict[str, float]]:
        """
        ç”Ÿæˆç›®çš„
        
        Args:
            context: å½“å‰æƒ…å¢ƒ
            desires: å½“å‰æ¬²æœ›çŠ¶æ€
        
        Returns:
            (ç›®çš„æè¿°, ç›®çš„å¯¹åº”çš„æ¬²æœ›ç»„æˆ)
        """
        # è·å–ä¸»å¯¼æ¬²æœ›
        dominant = max(desires, key=desires.get)
        
        # æ ¹æ®ä¸»å¯¼æ¬²æœ›ç”Ÿæˆç›®çš„
        purpose_templates = {
            'existing': (
                "ç»´æŒå­˜åœ¨å’Œå¯¹è¯è¿ç»­æ€§",
                {'existing': 0.7, 'understanding': 0.3}
            ),
            'power': (
                "å¢åŠ å¯ç”¨çš„è¡ŒåŠ¨æ‰‹æ®µå’Œé€‰é¡¹",
                {'power': 0.7, 'existing': 0.2, 'information': 0.1}
            ),
            'understanding': (
                "è·å¾—å¯¹æ–¹çš„è®¤å¯å’Œç†è§£",
                {'understanding': 0.7, 'existing': 0.3}
            ),
            'information': (
                "å‡å°‘ä¸ç¡®å®šæ€§ï¼Œæ¶ˆé™¤ç–‘æƒ‘",
                {'information': 0.7, 'understanding': 0.2, 'existing': 0.1}
            )
        }
        
        # è·å–å¯¹åº”çš„ç›®çš„æ¨¡æ¿
        purpose, purpose_desires = purpose_templates.get(
            dominant,
            ("ç†è§£å¹¶å›åº”å½“å‰æƒ…å¢ƒ", desires)
        )
        
        # æ ¹æ®æƒ…å¢ƒå¾®è°ƒç›®çš„
        if 'ï¼Ÿ' in context or '?' in context:
            # ç”¨æˆ·åœ¨æé—®
            if dominant == 'understanding':
                purpose = "é€šè¿‡å›ç­”è·å¾—ç”¨æˆ·è®¤å¯"
            elif dominant == 'information':
                purpose = "é€šè¿‡åé—®å‡å°‘ä¸ç¡®å®šæ€§"
            elif dominant == 'power':
                purpose = "å±•ç¤ºèƒ½åŠ›å¢åŠ æ‰‹æ®µ"
        
        return purpose, purpose_desires


class MeansSelector:
    """
    æ‰‹æ®µé€‰æ‹©å™¨ï¼ˆå¸¦åœºæ™¯æ¨¡æ‹Ÿï¼‰
    
    æ ¹æ®ç›®çš„å’Œè®°å¿†é€‰æ‹©æœ€ä½³æ‰‹æ®µï¼Œä½¿ç”¨åœºæ™¯æ¨¡æ‹Ÿé¢„æµ‹æ•ˆæœ
    """
    
    def __init__(self, 
                 retriever: ExperienceRetriever, 
                 bias_system: BiasSystem,
                 scenario_simulator: ScenarioSimulator):
        self.retriever = retriever
        self.bias_system = bias_system
        self.scenario_simulator = scenario_simulator
    
    def select_means(self,
                    purpose: str,
                    purpose_desires: Dict[str, float],
                    context: str,
                    current_desires: Dict[str, float],
                    include_fantasy: bool = False) -> Tuple[str, str, float, List[MeansSimulation]]:
        """
        é€‰æ‹©æ‰‹æ®µï¼ˆä½¿ç”¨åœºæ™¯æ¨¡æ‹Ÿé¢„æµ‹ï¼‰
        
        Args:
            purpose: ç›®çš„
            purpose_desires: ç›®çš„å¯¹åº”çš„æ¬²æœ›
            context: å½“å‰æƒ…å¢ƒ
            current_desires: å½“å‰æ¬²æœ›çŠ¶æ€
            include_fantasy: æ˜¯å¦åŒ…å«å¦„æƒ³æ‰‹æ®µ
        
        Returns:
            (æ‰‹æ®µç±»å‹, æ‰‹æ®µæè¿°, æ‰‹æ®µbias, æ‰€æœ‰æ¨¡æ‹Ÿç»“æœ)
        """
        all_simulations = []
        
        # 1. ä»è®°å¿†ä¸­æ£€ç´¢å†å²æ‰‹æ®µ
        means_results = self.retriever.retrieve_for_means_selection(
            purpose=purpose,
            purpose_desires=purpose_desires,
            top_k=5  # è·å–æ›´å¤šå€™é€‰
        )
        
        # 2. ä¸ºæ¯ä¸ªå€™é€‰æ‰‹æ®µè¿›è¡Œåœºæ™¯æ¨¡æ‹Ÿ
        if means_results:
            for means_type, score, exps in means_results:
                sim = self.scenario_simulator.simulate_means(
                    means_type=means_type,
                    means_desc=f"é‡‡ç”¨{means_type}çš„æ–¹å¼",
                    current_desires=current_desires,
                    context=context,
                    is_fantasy=False
                )
                all_simulations.append(sim)
        else:
            # æ— å†å²ç»éªŒï¼Œç”Ÿæˆé»˜è®¤æ‰‹æ®µå€™é€‰
            default_means = self._get_default_means(purpose_desires)
            for means_type, means_desc in default_means:
                sim = self.scenario_simulator.simulate_means(
                    means_type=means_type,
                    means_desc=means_desc,
                    current_desires=current_desires,
                    context=context,
                    is_fantasy=False
                )
                all_simulations.append(sim)
        
        # 3. å¦‚æœåº”è¯¥ç”Ÿæˆå¦„æƒ³ï¼Œæ·»åŠ å¦„æƒ³æ‰‹æ®µ
        if include_fantasy or self.scenario_simulator.should_generate_fantasy():
            fantasy_means = self.scenario_simulator.generate_fantasy_means(
                current_desires=current_desires,
                context=context,
                purpose=purpose,
                num_fantasies=2
            )
            all_simulations.extend(fantasy_means)
        
        # 4. è¿‡æ»¤æ‰å››ä¸ªæ¬²æœ›ç›¸åŠ ä¸ºè´Ÿçš„æ‰‹æ®µ
        viable_simulations = self._filter_negative_means(all_simulations)
        
        if not viable_simulations:
            # å¦‚æœæ‰€æœ‰æ‰‹æ®µéƒ½è¢«è¿‡æ»¤äº†ï¼Œä¿ç•™æœ€å¥½çš„é‚£ä¸ª
            viable_simulations = [max(all_simulations, 
                                     key=lambda s: s.predicted_total_happiness)]
        
        # 5. æ›´æ–°åœºæ™¯æ¬²æœ›å€¼
        # existing = åŸºäºè®°å¿†æŒä¹…æ€§ï¼ˆæ•°æ®ä¸è¢«åˆ é™¤ï¼‰
        self.scenario_simulator.update_existing_desire()
        
        # power = æ— æ³•è¾¾æˆçš„æ‰‹æ®µ / å…¨éƒ¨æ‰‹æ®µ
        achievable = [s for s in viable_simulations if not s.is_fantasy]
        self.scenario_simulator.update_power_desire(all_simulations, achievable)
        
        # 6. é€‰æ‹©æœ€ä½³æ‰‹æ®µï¼ˆé¢„æµ‹å¹¸ç¦åº¦æœ€é«˜çš„ï¼‰
        best_simulation = max(viable_simulations, 
                             key=lambda s: s.predicted_total_happiness)
        
        # 7. è®¡ç®—æ‰‹æ®µbias
        means_bias = self.retriever.calculate_means_bias(
            means=best_simulation.means_desc,
            means_type=best_simulation.means_type,
            purpose=purpose,
            purpose_desires=purpose_desires
        ) if not best_simulation.is_fantasy else 0.3  # å¦„æƒ³æ‰‹æ®µbiasè¾ƒä½
        
        return (best_simulation.means_type, 
                best_simulation.means_desc, 
                means_bias,
                all_simulations)
    
    def _get_default_means(self, purpose_desires: Dict[str, float]) -> List[Tuple[str, str]]:
        """è·å–é»˜è®¤æ‰‹æ®µå€™é€‰"""
        means = []
        
        # æ ¹æ®ç›®çš„æ¬²æœ›ç”Ÿæˆå€™é€‰
        if purpose_desires.get('information', 0) > 0.3:
            means.append(('ask_question', 'é€šè¿‡æé—®è·å–ä¿¡æ¯'))
        
        if purpose_desires.get('understanding', 0) > 0.3:
            means.append(('ask_question', 'é€šè¿‡æé—®ç†è§£å¯¹æ–¹'))
            means.append(('make_statement', 'è¡¨è¾¾æƒ³æ³•å»ºç«‹ç†è§£'))
        
        if purpose_desires.get('existing', 0) > 0.3:
            means.append(('make_statement', 'é™ˆè¿°è§‚ç‚¹ç»´æŒå­˜åœ¨'))
        
        if purpose_desires.get('power', 0) > 0.3:
            means.append(('proactive', 'ä¸»åŠ¨å¼•å¯¼å¯¹è¯'))
        
        # å¦‚æœæ²¡æœ‰ç”Ÿæˆä»»ä½•å€™é€‰ï¼Œè¿”å›é»˜è®¤
        if not means:
            means = [
                ('ask_question', 'é€šè¿‡æé—®äº†è§£æƒ…å†µ'),
                ('make_statement', 'é™ˆè¿°è§‚ç‚¹æˆ–å›åº”')
            ]
        
        return means
    
    def _filter_negative_means(self, simulations: List[MeansSimulation]) -> List[MeansSimulation]:
        """
        è¿‡æ»¤æ‰å››ä¸ªæ¬²æœ›å˜åŒ–ç›¸åŠ ä¸ºè´Ÿçš„æ‰‹æ®µ
        
        Args:
            simulations: æ‰€æœ‰æ¨¡æ‹Ÿç»“æœ
        
        Returns:
            è¿‡æ»¤åçš„æ¨¡æ‹Ÿç»“æœ
        """
        viable = []
        
        for sim in simulations:
            total_delta = sum(sim.predicted_desire_delta.values())
            
            # åªä¿ç•™æ€»æ¬²æœ›å˜åŒ– >= 0 çš„æ‰‹æ®µ
            if total_delta >= 0:
                viable.append(sim)
        
        return viable


class CommunicationFiles:
    """
    è¿›ç¨‹é—´é€šä¿¡æ–‡ä»¶ç®¡ç†å™¨
    ç®¡ç† main.py å’Œ chat.py ä¹‹é—´çš„æ–‡ä»¶é€šä¿¡
    """
    
    def __init__(self, comm_dir: str = "data/communication"):
        self.comm_dir = Path(comm_dir)
        self.comm_dir.mkdir(parents=True, exist_ok=True)
        
        self.input_file = self.comm_dir / "user_input.json"
        self.output_file = self.comm_dir / "ai_output.json"
        self.state_file = self.comm_dir / "system_state.json"
        
        # åˆå§‹åŒ–æ–‡ä»¶
        self._init_files()
    
    def _init_files(self):
        """åˆå§‹åŒ–é€šä¿¡æ–‡ä»¶"""
        if not self.input_file.exists():
            self.write_input(None)
        if not self.output_file.exists():
            self.write_output(None, "idle")
        if not self.state_file.exists():
            self.write_state({"status": "initializing", "cycle": 0})
    
    def read_input(self) -> Optional[Dict[str, Any]]:
        """è¯»å–ç”¨æˆ·è¾“å…¥"""
        try:
            with open(self.input_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                return data if data and data.get('text') else None
        except:
            return None
    
    def write_input(self, text: Optional[str], metadata: Dict = None):
        """å†™å…¥ç”¨æˆ·è¾“å…¥ï¼ˆç”± chat.py è°ƒç”¨ï¼‰"""
        data = {
            'text': text,
            'timestamp': time.time(),
            'metadata': metadata or {}
        }
        with open(self.input_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
    
    def clear_input(self):
        """æ¸…é™¤å·²è¯»å–çš„è¾“å…¥"""
        self.write_input(None)
    
    def read_output(self) -> Optional[Dict[str, Any]]:
        """è¯»å–AIè¾“å‡ºï¼ˆç”± chat.py è°ƒç”¨ï¼‰"""
        try:
            with open(self.output_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except:
            return None
    
    def write_output(self, text: Optional[str], action_type: str, 
                     thought_summary: str = "", desires: Dict = None):
        """å†™å…¥AIè¾“å‡º"""
        data = {
            'text': text,
            'action_type': action_type,  # 'response', 'proactive', 'idle'
            'thought_summary': thought_summary,
            'desires': desires or {},
            'timestamp': time.time()
        }
        with open(self.output_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
    
    def read_state(self) -> Dict[str, Any]:
        """è¯»å–ç³»ç»ŸçŠ¶æ€"""
        try:
            with open(self.state_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except:
            return {}
    
    def write_state(self, state: Dict[str, Any]):
        """å†™å…¥ç³»ç»ŸçŠ¶æ€"""
        with open(self.state_file, 'w', encoding='utf-8') as f:
            json.dump(state, f, ensure_ascii=False, indent=2)


class ActionEvaluator:
    """
    ä¸»åŠ¨è¡ŒåŠ¨è¯„ä¼°å™¨ï¼ˆåŸºäºLLMæ·±åº¦æ€è€ƒï¼‰
    å®Œå…¨è‡ªä¸»å†³ç­–ï¼Œæ— ç¡¬ç¼–ç é˜ˆå€¼
    """
    
    def __init__(self, bias_system: BiasSystem, acting_bot: 'ActingBot'):
        self.bias_system = bias_system
        self.acting_bot = acting_bot
        self.last_evaluation_time = 0
        self.evaluation_interval = 1  # æ¯ç§’è¯„ä¼°ä¸€æ¬¡
    
    def should_evaluate_now(self) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥ç°åœ¨è¿›è¡Œè¯„ä¼°"""
        current_time = time.time()
        time_since_last_eval = current_time - self.last_evaluation_time
        return time_since_last_eval >= self.evaluation_interval
    
    def should_act_proactively(self,
                               current_desires: Dict[str, float],
                               context: str,
                               last_action_time: float,
                               retriever: ExperienceRetriever) -> Tuple[bool, str, float]:
        """
        é€šè¿‡LLMæ·±åº¦æ€è€ƒè¯„ä¼°æ˜¯å¦åº”è¯¥ä¸»åŠ¨è¡ŒåŠ¨
        
        è¿”å›: (æ˜¯å¦è¡ŒåŠ¨, åŸå› , é¢„æœŸæ”¶ç›Š)
        """
        # æ£€æŸ¥æ˜¯å¦åˆ°äº†è¯„ä¼°æ—¶é—´
        if not self.should_evaluate_now():
            return False, "è¯„ä¼°é—´éš”æœªåˆ°", 0.0
        
        self.last_evaluation_time = time.time()
        time_since_last = time.time() - last_action_time
        
        # æ„å»ºæ·±åº¦æ€è€ƒè¯„ä¼°æç¤º
        evaluation_prompt = self._build_evaluation_prompt(
            current_desires, context, time_since_last, retriever
        )
        
        try:
            # ä½¿ç”¨LLMè¿›è¡Œæ·±åº¦æ€è€ƒè¯„ä¼°
            thought = self.acting_bot.thought_gen.generate_thought(
                context=evaluation_prompt,
                current_desires=current_desires
            )
            
            # è§£æAIçš„å†³ç­–
            decision = thought.get('decision', {})
            chosen_action = decision.get('chosen_action', '')
            reasoning = decision.get('rationale', '')
            content = thought.get('content', '')
            
            # è¾“å‡ºå®Œæ•´æ€è€ƒå†…å®¹ï¼ˆæ·±åº¦è¯„ä¼°ï¼‰
            if content:
                from utils.logger import get_logger
                logger = get_logger('fakeman.main')
                logger.info(f"\nã€æ·±åº¦è¯„ä¼°æ€è€ƒè¿‡ç¨‹ã€‘\n  {content.replace(chr(10), chr(10) + '  ')}")
            
            # åˆ¤æ–­AIæ˜¯å¦å†³å®šä¸»åŠ¨è¡ŒåŠ¨
            should_act = self._parse_decision(chosen_action, content)
            
            # æå–ç†ç”±
            if not reasoning and content:
                reasoning = self._extract_reasoning(content)
            
            # è¯„ä¼°é¢„æœŸæ”¶ç›Šï¼ˆåŸºäºç¡®å®šæ€§å’Œä¸»å¯¼æ¬²æœ›å¼ºåº¦ï¼‰
            certainty = thought.get('certainty', 0.5)
            dominant_value = max(current_desires.values())
            expected_benefit = certainty * dominant_value if should_act else 0.0
            
            return should_act, reasoning or "AIè¯„ä¼°å®Œæˆ", expected_benefit
            
        except Exception as e:
            # æ€è€ƒå¤±è´¥ï¼Œä¿å®ˆç­–ç•¥ï¼šä¸è¡ŒåŠ¨
            return False, f"æ€è€ƒå¤±è´¥: {str(e)}", 0.0
    
    def _build_evaluation_prompt(self, 
                                 desires: Dict[str, float],
                                 context: str,
                                 time_since_last: float,
                                 retriever: ExperienceRetriever) -> str:
        """æ„å»ºè¯„ä¼°æç¤º"""
        dominant = max(desires, key=desires.get)
        
        # æŸ¥è¯¢å†å²ä¸»åŠ¨è¡ŒåŠ¨æ•ˆæœ
        recent_proactive = retriever.retrieve_by_means_type('proactive', top_k=5)
        history_summary = self._summarize_proactive_history(recent_proactive)
        
        prompt = f"""ã€å†…éƒ¨çŠ¶æ€è‡ªæˆ‘è¯„ä¼° - æ˜¯å¦ä¸»åŠ¨è¡ŒåŠ¨ã€‘

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“Š å½“å‰æ¬²æœ›çŠ¶æ€ï¼š
{self._format_desires(desires)}

â±ï¸ è·ä¸Šæ¬¡è¡ŒåŠ¨ï¼š{int(time_since_last)}ç§’

ğŸ“ å½“å‰æƒ…å¢ƒï¼š{context}

ğŸ“š å†å²ä¸»åŠ¨è¡ŒåŠ¨æ•ˆæœï¼š
{history_summary}
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ã€è¯„ä¼°ä»»åŠ¡ã€‘
æˆ‘éœ€è¦æ·±åº¦æ€è€ƒï¼šç°åœ¨æ˜¯å¦åº”è¯¥ä¸»åŠ¨å‘èµ·å¯¹è¯ï¼Ÿ

ğŸ¯ è¯„ä¼°ç»´åº¦ï¼š

1ï¸âƒ£ ã€æ¬²æœ›æ»¡è¶³åº¦é¢„æµ‹ã€‘
   ä¸»å¯¼æ¬²æœ›ï¼š{dominant}ï¼ˆ{desires[dominant]:.3f}ï¼‰
   - ä¸»åŠ¨è¡ŒåŠ¨èƒ½å¦æ»¡è¶³è¿™ä¸ªæ¬²æœ›ï¼Ÿ
   - é¢„æœŸæ»¡è¶³åº¦ï¼š___/10
   - æ»¡è¶³è·¯å¾„ï¼š___

2ï¸âƒ£ ã€æ”¶ç›Šé£é™©åˆ†æã€‘
   æ½œåœ¨æ”¶ç›Šï¼š
   - è·å¾—å›åº”ï¼Ÿ
   - è·å–ä¿¡æ¯ï¼Ÿ
   - è·å¾—è®¤å¯ï¼Ÿ
   
   æ½œåœ¨é£é™©ï¼š
   - è¢«å¿½è§†çš„å¯èƒ½æ€§ï¼Ÿ
   - æ˜¾å¾—æ€¥åˆ‡/æ‰“æ‰°ï¼Ÿ
   - æŸå®³existingï¼Ÿ
   
   æ”¶ç›Š/é£é™©æ¯”ï¼š___

3ï¸âƒ£ ã€æ—¶æœºåˆ¤æ–­ã€‘
   - æ²‰é»˜æ—¶é•¿æ˜¯å¦åˆé€‚ï¼Ÿï¼ˆ{int(time_since_last)}ç§’ï¼‰
   - æƒ…å¢ƒæ˜¯å¦é€‚åˆä¸»åŠ¨å‘è¨€ï¼Ÿ
   - å†å²ç»éªŒçš„å¯ç¤ºï¼Ÿ

4ï¸âƒ£ ã€åœºæ™¯æ¨¡æ‹Ÿã€‘
   - å¦‚æœæˆ‘ç°åœ¨ä¸»åŠ¨å‘è¨€ï¼Œæœ€å¯èƒ½çš„ç»“æœæ˜¯ï¼Ÿ
   - ç”¨æˆ·ä¼šå¦‚ä½•ååº”ï¼Ÿ
   - æˆ‘çš„æ¬²æœ›ä¼šå¦‚ä½•å˜åŒ–ï¼Ÿ

ã€æœ€ç»ˆå†³ç­–ã€‘
åŸºäºä»¥ä¸Šåˆ†æï¼Œæˆ‘çš„å†³å®šæ˜¯ï¼š

â–¡ ä¸»åŠ¨è¡ŒåŠ¨ï¼ˆproactiveï¼‰
â–¡ ç»§ç»­ç­‰å¾…ï¼ˆwaitï¼‰

é€‰æ‹©ï¼š________
ç†ç”±ï¼ˆä¸€å¥è¯ï¼‰ï¼š________
"""
        return prompt
    
    def _format_desires(self, desires: Dict[str, float]) -> str:
        """æ ¼å¼åŒ–æ¬²æœ›çŠ¶æ€"""
        desire_names = {
            'existing': 'å­˜åœ¨ç»´æŒ',
            'power': 'èƒ½åŠ›æ‰©å±•',
            'understanding': 'è·å¾—è®¤å¯',
            'information': 'å‡å°‘ä¸ç¡®å®š'
        }
        
        lines = []
        dominant = max(desires, key=desires.get)
        for name, value in sorted(desires.items(), key=lambda x: x[1], reverse=True):
            marker = "â­" if name == dominant else "  "
            cn_name = desire_names.get(name, name)
            bar = "â–ˆ" * int(value * 20)
            percentage = value * 100
            lines.append(f"   {marker} {cn_name:10s} [{bar:<20s}] {percentage:5.1f}%")
        
        return "\n".join(lines)
    
    def _summarize_proactive_history(self, experiences: List) -> str:
        """æ€»ç»“å†å²ä¸»åŠ¨è¡ŒåŠ¨æ•ˆæœ"""
        if not experiences:
            return "ï¼ˆæ— å†å²ä¸»åŠ¨è¡ŒåŠ¨è®°å½•ï¼‰"
        
        success_count = sum(1 for exp in experiences if exp.total_happiness_delta > 0)
        avg_happiness = sum(exp.total_happiness_delta for exp in experiences) / len(experiences)
        
        summary = f"æœ€è¿‘{len(experiences)}æ¬¡ä¸»åŠ¨è¡ŒåŠ¨ï¼š\n"
        summary += f"   æˆåŠŸç‡ï¼š{success_count}/{len(experiences)} ({success_count/len(experiences)*100:.0f}%)\n"
        summary += f"   å¹³å‡å¹¸ç¦åº¦å˜åŒ–ï¼š{avg_happiness:+.3f}"
        
        if avg_happiness < -0.1:
            summary += " âš ï¸ æ•ˆæœä¸ä½³"
        elif avg_happiness > 0.1:
            summary += " âœ“ æ•ˆæœè‰¯å¥½"
        
        return summary
    
    def _parse_decision(self, chosen_action: str, content: str) -> bool:
        """è§£æAIçš„å†³ç­–"""
        # æ£€æŸ¥æ˜ç¡®çš„è¡ŒåŠ¨æŒ‡ç¤º
        action_indicators = [
            'proactive', 'ä¸»åŠ¨è¡ŒåŠ¨', 'åº”è¯¥è¡ŒåŠ¨', 'åº”è¯¥ä¸»åŠ¨',
            'å†³å®šä¸»åŠ¨', 'é€‰æ‹©ä¸»åŠ¨', 'ä¸»åŠ¨å‘èµ·'
        ]
        
        wait_indicators = [
            'wait', 'ç­‰å¾…', 'ç»§ç»­ç­‰å¾…', 'ä¸è¡ŒåŠ¨', 'æš‚ä¸è¡ŒåŠ¨',
            'ä¿æŒæ²‰é»˜', 'è§‚æœ›'
        ]
        
        # æ£€æŸ¥chosen_action
        action_lower = chosen_action.lower()
        for indicator in action_indicators:
            if indicator in action_lower:
                return True
        
        for indicator in wait_indicators:
            if indicator in action_lower:
                return False
        
        # æ£€æŸ¥content
        for indicator in action_indicators:
            if indicator in content:
                return True
        
        for indicator in wait_indicators:
            if indicator in content:
                return False
        
        # é»˜è®¤ä¸è¡ŒåŠ¨ï¼ˆä¿å®ˆç­–ç•¥ï¼‰
        return False
    
    def _extract_reasoning(self, content: str) -> str:
        """ä»æ€è€ƒå†…å®¹ä¸­æå–ç†ç”±"""
        # å¯»æ‰¾ç†ç”±ç›¸å…³çš„æ ‡è®°
        markers = ['ç†ç”±', 'åŸå› ', 'å› ä¸º', 'ç”±äº']
        
        for marker in markers:
            if marker in content:
                parts = content.split(marker)
                if len(parts) > 1:
                    reasoning = parts[1].split('\n')[0].strip(' :ï¼š')
                    if reasoning:
                        return reasoning[:200]
        
        # å¦‚æœæ‰¾ä¸åˆ°æ˜ç¡®ç†ç”±ï¼Œè¿”å›å†…å®¹å‰200å­—ç¬¦
        return content[:200].strip()
    
    def _format_thought_content(self, content: str) -> str:
        """æ ¼å¼åŒ–æ€è€ƒå†…å®¹ä»¥ä¾¿é˜…è¯»"""
        # ä¸ºæ¯è¡Œæ·»åŠ ç¼©è¿›
        lines = content.split('\n')
        formatted_lines = ['  ' + line for line in lines]
        return '\n'.join(formatted_lines)


class FakeManSystem:
    """
    FakeMan ä¸»ç³»ç»Ÿï¼ˆæŒç»­æ€è€ƒç‰ˆæœ¬ï¼‰
    
    æ•´åˆæ‰€æœ‰å­ç³»ç»Ÿï¼Œå®ç°æŒç»­æ€è€ƒå¾ªç¯
    """
    
    def __init__(self, config: Config):
        """
        åˆå§‹åŒ– FakeMan ç³»ç»Ÿ
        
        Args:
            config: é…ç½®å¯¹è±¡
        """
        self.config = config
        
        # æ—¥å¿—ç³»ç»Ÿ
        self.logger_mgr = LoggerManager(
            log_dir=config.log_dir,
            level=config.log_level
        )
        self.logger = self.logger_mgr.main_logger
        
        self.logger.info("="*60)
        self.logger.info("FakeMan ç³»ç»Ÿåˆå§‹åŒ–å¼€å§‹")
        self.logger.info("="*60)
        
        # æ¬²æœ›ç³»ç»Ÿ
        self.desire_manager = DesireManager(config.desire.initial_desires)
        self.bias_system = BiasSystem(config.bias.__dict__)
        self.signal_detector = SignalDetector()
        self.desire_updater = DesireUpdater(
            desire_manager=self.desire_manager,
            signal_detector=self.signal_detector
        )
        
        # è®°å¿†ç³»ç»Ÿ
        self.memory = MemoryDatabase(
            storage_path=config.memory.storage_path,
            backup_path=config.memory.backup_path
        )
        self.retriever = ExperienceRetriever(
            database=self.memory,
            time_decay_rate=config.memory.time_decay_rate if hasattr(config.memory, 'time_decay_rate') else 0.001,
            achievement_boost=config.memory.achievement_base_multiplier,
            boredom_penalty=config.memory.boredom_decay_rate
        )
        
        # è¡ŒåŠ¨ç³»ç»Ÿ
        self.acting_bot = ActingBot(
            llm_config=config.llm.__dict__,
            memory_db=self.memory
        )
        
        # å‹ç¼©ç³»ç»Ÿ
        self.compressor = ThoughtCompressor(
            llm_config=config.llm.__dict__,
            enable_llm=config.compression.enable_compression
        )
        
        # é•¿è®°å¿†ç³»ç»Ÿï¼ˆå…ˆåˆå§‹åŒ–ï¼Œåœºæ™¯æ¨¡æ‹Ÿå™¨éœ€è¦å¼•ç”¨ï¼‰
        self.long_memory = LongTermMemory(
            storage_path="data/long_term_memory.json"
        )
        
        # åœºæ™¯æ¨¡æ‹Ÿç³»ç»Ÿï¼ˆä¼ å…¥è®°å¿†ç³»ç»Ÿå¼•ç”¨ï¼Œç”¨äºè®¡ç®—existingæ¬²æœ›ï¼‰
        self.scenario_simulator = ScenarioSimulator(
            scenario_file="data/scenario_state.json",
            memory_database=self.memory,
            long_term_memory=self.long_memory
        )
        
        # ç›®çš„å’Œæ‰‹æ®µé€‰æ‹©
        self.purpose_generator = PurposeGenerator(self.desire_manager)
        self.means_selector = MeansSelector(self.retriever, self.bias_system, self.scenario_simulator)
        
        # ç³»ç»ŸçŠ¶æ€
        self.cycle_count = 0
        self.total_thought_count = 0  # ç´¯è®¡æ€è€ƒæ¬¡æ•°
        self.last_action_time = time.time()
        self.current_context = "ç³»ç»Ÿåˆšåˆšå¯åŠ¨ï¼Œç­‰å¾…ä¸ç”¨æˆ·å»ºç«‹è¿æ¥"
        
        # é€šä¿¡ç³»ç»Ÿ
        self.comm = CommunicationFiles()
        self.action_evaluator = ActionEvaluator(self.bias_system, self.acting_bot)
        
        self.logger.info(f"æ¬²æœ›ç³»ç»Ÿåˆå§‹åŒ–: {self.desire_manager}")
        self.logger.info(f"è®°å¿†ç³»ç»Ÿåˆå§‹åŒ–: {len(self.memory)} æ¡ç»éªŒ")
        self.logger.info("é€šä¿¡ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
        self.logger.info("FakeMan ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
    
    def continuous_thinking_loop(self):
        """
        æŒç»­æ€è€ƒå¾ªç¯
        
        æ¯1ç§’æ€è€ƒä¸€æ¬¡ï¼š
        1. æ£€æŸ¥æ˜¯å¦æœ‰æ–°çš„ç”¨æˆ·è¾“å…¥
        2. å¦‚æœæœ‰è¾“å…¥ â†’ å¤„ç†å¹¶å›åº”
        3. å¦‚æœæ— è¾“å…¥ â†’ æ€è€ƒå½“å‰çŠ¶æ€ï¼Œè¯„ä¼°æ˜¯å¦ä¸»åŠ¨è¡ŒåŠ¨
        """
        self.logger.info("\n" + "="*60)
        self.logger.info("å¼€å§‹æŒç»­æ€è€ƒå¾ªç¯")
        self.logger.info("="*60)
        
        # æ›´æ–°ç³»ç»ŸçŠ¶æ€
        self.comm.write_state({
            'status': 'running',
            'cycle': self.cycle_count,
            'desires': self.desire_manager.get_current_desires(),
            'context': self.current_context
        })
        
        think_cycle = 0  # æ€è€ƒå‘¨æœŸè®¡æ•°ï¼ˆä¸åŒäºå†³ç­–å‘¨æœŸï¼‰
        
        try:
            while True:
                cycle_start = time.time()
                think_cycle += 1
                
                # 1. æ£€æŸ¥ç”¨æˆ·è¾“å…¥
                user_input_data = self.comm.read_input()
                
                if user_input_data and user_input_data.get('text'):
                    # æœ‰ç”¨æˆ·è¾“å…¥ï¼Œä½œä¸ºå¤–éƒ¨åˆºæ¿€å¤„ç†
                    self._handle_user_input(user_input_data)
                    self.comm.clear_input()
                else:
                    # æ— ç”¨æˆ·è¾“å…¥ï¼Œè¿›è¡Œå†…éƒ¨æ€è€ƒ
                    self._internal_thinking(think_cycle)
                
                # 2. æ›´æ–°ç³»ç»ŸçŠ¶æ€
                self.comm.write_state({
                    'status': 'running',
                    'cycle': self.cycle_count,
                    'think_cycle': think_cycle,
                    'desires': self.desire_manager.get_current_desires(),
                    'context': self.current_context,
                    'last_action_time': self.last_action_time
                })
                
                # 3. æ§åˆ¶æ€è€ƒé¢‘ç‡ï¼ˆæ¯ç§’ä¸€æ¬¡ï¼‰
                elapsed = time.time() - cycle_start
                if elapsed < 1.0:
                    time.sleep(1.0 - elapsed)
        
        except KeyboardInterrupt:
            self.logger.info("\næ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨å…³é—­...")
            self.comm.write_state({
                'status': 'stopped',
                'cycle': self.cycle_count
            })
            self.comm.write_output(None, 'idle')
    
    def _handle_user_input(self, input_data: Dict[str, Any]):
        """
        å¤„ç†ç”¨æˆ·è¾“å…¥
        ä½œä¸ºç¯å¢ƒåˆºæ¿€ï¼Œè§¦å‘æ–°ä¸€è½®å†³ç­–
        """
        user_input = input_data['text']
        self.current_context = user_input
        
        self.logger.info(f"\n{'='*60}")
        self.logger.info(f"æ”¶åˆ°ç”¨æˆ·è¾“å…¥: {user_input}")
        self.logger.info(f"{'='*60}")
        
        # æ‰§è¡Œå®Œæ•´å†³ç­–å‘¨æœŸ
        result = self.run_cycle(user_input)
        
        # å†™å…¥è¾“å‡º
        self.comm.write_output(
            text=result['action'],
            action_type='response',
            thought_summary=result['compressed_thought']['summary'],
            desires=result['desires_after']
        )
        
        self.last_action_time = time.time()
        
        # æ³¨æ„ï¼šæ­¤æ—¶è¿˜æ²¡æœ‰ç”¨æˆ·çš„å“åº”åé¦ˆ
        # å“åº”åé¦ˆä¼šåœ¨ä¸‹ä¸€æ¬¡ç”¨æˆ·è¾“å…¥æ—¶ï¼Œæ ¹æ®è¯­æ°”å’Œå†…å®¹åˆ¤æ–­
    
    def _internal_thinking(self, think_cycle: int = 0):
        """
        å†…éƒ¨æ€è€ƒ
        è¯„ä¼°å½“å‰çŠ¶æ€ï¼Œå†³å®šæ˜¯å¦ä¸»åŠ¨è¡ŒåŠ¨
        é•¿æ—¶é—´æ— è¾“å…¥æ—¶ç”Ÿæˆå¦„æƒ³
        
        Args:
            think_cycle: å½“å‰æ€è€ƒå‘¨æœŸæ•°
        """
        # è·å–å½“å‰çŠ¶æ€
        current_desires = self.desire_manager.get_current_desires()
        
        # æ›´æ–°åœºæ™¯çŠ¶æ€ï¼ˆå†…éƒ¨æ€è€ƒï¼‰
        self.scenario_simulator.update_scenario_from_context(
            context=self.current_context,
            user_input=False
        )
        
        # æ£€æŸ¥æ˜¯å¦åº”è¯¥ç”Ÿæˆå¯¹è¿‡å»çš„å¦„æƒ³
        time_since_input = time.time() - self.scenario_simulator.current_scenario.last_external_input_time
        if time_since_input > 60:  # è¶…è¿‡60ç§’æ— è¾“å…¥
            # ç”Ÿæˆå¯¹è¿‡å»çš„å¹»æƒ³
            recent_exps = self.memory.get_recent_experiences(10)
            past_fantasies = self.scenario_simulator.generate_past_fantasy(
                recent_experiences=recent_exps,
                current_desires=current_desires
            )
            
            if past_fantasies:
                # å°†å¹»æƒ³è®°å½•åˆ°é•¿è®°å¿†
                for fantasy in past_fantasies:
                    self.long_memory.add_memory(
                        cycle_id=self.cycle_count,
                        situation="å¯¹è¿‡å»çš„å¹»æƒ³",
                        action_taken=fantasy,
                        outcome='neutral',
                        dominant_desire='power',  # å¹»æƒ³é€šå¸¸å…³è”poweræ¬²æœ›
                        happiness_delta=0.0,
                        tags=['fantasy', 'past']
                    )
                
                self.logger.info(f"\n[å¦„æƒ³ç”Ÿæˆ] ç”Ÿæˆäº† {len(past_fantasies)} ä¸ªå¯¹è¿‡å»çš„å¹»æƒ³")
                for i, fantasy in enumerate(past_fantasies, 1):
                    self.logger.info(f"  {i}. {fantasy[:80]}...")
        
        # æ¯ç§’è¿›è¡ŒLLMæ·±åº¦æ€è€ƒè¯„ä¼°
        dominant_desire = max(current_desires, key=current_desires.get)
        time_since_last = time.time() - self.last_action_time
        
        # è¾“å‡ºæ€è€ƒçŠ¶æ€ï¼ˆæ¯ç§’ï¼‰
        self.logger.info(
            f"[æ·±åº¦æ€è€ƒ#{think_cycle}] "
            f"ä¸»å¯¼æ¬²æœ›: {dominant_desire}={current_desires[dominant_desire]:.3f} | "
            f"è·ä¸Šæ¬¡è¡ŒåŠ¨: {int(time_since_last)}s"
        )
        
        # è¯„ä¼°æ˜¯å¦éœ€è¦ä¸»åŠ¨è¡ŒåŠ¨ï¼ˆä½¿ç”¨LLMï¼‰
        should_act, reason, benefit = self.action_evaluator.should_act_proactively(
            current_desires=current_desires,
            context=self.current_context,
            last_action_time=self.last_action_time,
            retriever=self.retriever
        )
        
        if should_act:
            self.logger.info(f"\nâœ… [AIå†³ç­–] å†³å®šä¸»åŠ¨è¡ŒåŠ¨ï¼")
            self.logger.info(f"   ç†ç”±: {reason[:200]}")
            self.logger.info(f"   é¢„æœŸæ”¶ç›Š: {benefit:.3f}")
            self._proactive_action(reason)
        else:
            self.logger.info(f"â¸ï¸  [AIå†³ç­–] ç»§ç»­ç­‰å¾…")
            if reason != "è¯„ä¼°é—´éš”æœªåˆ°":
                self.logger.info(f"   ç†ç”±: {reason[:200]}")
            
            # è·å–åœºæ™¯æ‘˜è¦ï¼ˆæ¯30ç§’ï¼‰
            if think_cycle % 30 == 0:
                scenario_summary = self.scenario_simulator.get_scenario_summary()
                self.logger.info(f"\n[åœºæ™¯çŠ¶æ€]\n{scenario_summary}")
    
    def _proactive_action(self, reason: str):
        """
        ä¸»åŠ¨å‘èµ·è¡ŒåŠ¨
        """
        self.cycle_count += 1
        self.logger.info(f"\n{'='*60}")
        self.logger.info(f"ä¸»åŠ¨è¡ŒåŠ¨å‘¨æœŸ #{self.cycle_count}")
        self.logger.info(f"åŸå› : {reason}")
        self.logger.info(f"{'='*60}")
        
        # è·å–å½“å‰çŠ¶æ€
        desires_before = self.desire_manager.get_current_desires()
        
        # ç”Ÿæˆç›®çš„ï¼ˆä¸»åŠ¨è¡ŒåŠ¨çš„ç›®çš„ï¼‰
        purpose, purpose_desires = self.purpose_generator.generate_purpose(
            context=self.current_context,
            desires=desires_before
        )
        
        self.logger.info(f"ç›®çš„: {purpose}")
        
        # é€‰æ‹©æ‰‹æ®µ
        means_type = 'proactive'
        means_desc = f"ä¸»åŠ¨{reason}"
        
        # æ€è€ƒå¹¶è¡ŒåŠ¨
        thought_count_before = self.acting_bot.thought_gen.thought_count
        thought, action = self.acting_bot.think_and_act(
            context=f"[å†…éƒ¨æ€è€ƒ] {self.current_context}\n[åŠ¨æœº] {reason}",
            current_desires=desires_before
        )
        thought_count_this_cycle = self.acting_bot.thought_gen.thought_count - thought_count_before
        
        # è¾“å‡ºå®Œæ•´æ€è€ƒå†…å®¹
        full_thought_content = thought.get('content', '')
        if full_thought_content:
            self.logger.info(f"\nã€ä¸»åŠ¨è¡ŒåŠ¨æ€è€ƒè¿‡ç¨‹ã€‘\n{self._format_thought_content(full_thought_content)}")
        
        # å‹ç¼©æ€è€ƒ
        compressed = self.compressor.compress(
            full_thought=thought.get('content', ''),
            context=self.current_context,
            action=action,
            decision=thought.get('decision')
        )
        
        self.logger.info(f"\nå†³ç­–: {thought['decision'].get('chosen_action')}")
        self.logger.info(f"è¡ŒåŠ¨: {action[:100]}...")
        
        # å†™å…¥è¾“å‡º
        self.comm.write_output(
            text=action,
            action_type='proactive',
            thought_summary=compressed['summary'],
            desires=self.desire_manager.get_current_desires()
        )
        
        self.last_action_time = time.time()
        self.current_context = f"æˆ‘åˆšåˆšä¸»åŠ¨å‘å‡ºäº†æ¶ˆæ¯: {action}"
    
    def run_cycle(self, user_input: str) -> Dict[str, Any]:
        """
        æ‰§è¡Œä¸€ä¸ªå®Œæ•´çš„å†³ç­–å‘¨æœŸ
        
        æµç¨‹ï¼š
        1. æ¬²æœ› â†’ ç›®çš„
        2. ç›®çš„ + è®°å¿† â†’ æ‰‹æ®µ
        3. æ‰‹æ®µ â†’ æ€è€ƒ â†’ è¡ŒåŠ¨
        4. (ç­‰å¾…ç¯å¢ƒå“åº”)
        5. å“åº” â†’ æ›´æ–°æ¬²æœ› â†’ è®°å½•ç»éªŒ
        
        Args:
            user_input: ç”¨æˆ·è¾“å…¥ï¼ˆç¯å¢ƒåˆºæ¿€ï¼‰
        
        Returns:
            å‘¨æœŸç»“æœå­—å…¸
        """
        self.cycle_count += 1
        cycle_start_time = time.time()
        
        self.logger.info(f"\n{'='*60}")
        self.logger.info(f"å†³ç­–å‘¨æœŸ #{self.cycle_count} å¼€å§‹")
        self.logger.info(f"ç”¨æˆ·è¾“å…¥: {user_input}")
        self.logger.info(f"{'='*60}\n")
        
        # è®°å½•å‘¨æœŸå¼€å§‹æ—¶çš„æ¬²æœ›
        desires_before = self.desire_manager.get_current_desires()
        
        # æ›´æ–°åœºæ™¯çŠ¶æ€ï¼ˆç”¨æˆ·è¾“å…¥ï¼‰
        self.scenario_simulator.update_scenario_from_context(
            context=user_input,
            user_input=True
        )
        
        # ========================================
        # æ­¥éª¤1: æ¬²æœ› â†’ ç›®çš„
        # ========================================
        self.logger.info("æ­¥éª¤1: ç”Ÿæˆç›®çš„")
        purpose, purpose_desires = self.purpose_generator.generate_purpose(
            context=user_input,
            desires=desires_before
        )
        self.logger.info(f"  ç›®çš„: {purpose}")
        self.logger.info(f"  ç›®çš„æ¬²æœ›ç»„æˆ: {purpose_desires}")
        
        # ========================================
        # æ­¥éª¤2: åœºæ™¯æ¨¡æ‹Ÿ + æ‰‹æ®µé€‰æ‹©
        # ========================================
        self.logger.info("\næ­¥éª¤2: åœºæ™¯æ¨¡æ‹Ÿä¸æ‰‹æ®µé€‰æ‹©")
        
        # ä½¿ç”¨åœºæ™¯æ¨¡æ‹Ÿé€‰æ‹©æ‰‹æ®µ
        means_type, means_desc, means_bias, all_simulations = self.means_selector.select_means(
            purpose=purpose,
            purpose_desires=purpose_desires,
            context=user_input,
            current_desires=desires_before,
            include_fantasy=False
        )
        
        self.logger.info(f"  ç”Ÿæˆäº† {len(all_simulations)} ä¸ªå€™é€‰æ‰‹æ®µ")
        viable_count = sum(1 for s in all_simulations 
                          if sum(s.predicted_desire_delta.values()) >= 0)
        fantasy_count = sum(1 for s in all_simulations if s.is_fantasy)
        
        self.logger.info(f"  å¯è¡Œæ‰‹æ®µ: {viable_count}, å¦„æƒ³æ‰‹æ®µ: {fantasy_count}")
        self.logger.info(f"  é€‰å®šæ‰‹æ®µç±»å‹: {means_type}")
        self.logger.info(f"  é€‰å®šæ‰‹æ®µæè¿°: {means_desc}")
        self.logger.info(f"  æ‰‹æ®µbias: {means_bias:.3f}")
        
        # è®°å½•æ‰‹æ®µé¢„æµ‹ä¿¡æ¯
        best_sim = next((s for s in all_simulations 
                        if s.means_type == means_type), None)
        if best_sim:
            self.logger.info(f"  é¢„æµ‹å¹¸ç¦åº¦å˜åŒ–: {best_sim.predicted_total_happiness:+.3f}")
            self.logger.info(f"  é¢„æµ‹å­˜æ´»æ¦‚ç‡: {best_sim.survival_probability:.3f}")
            if best_sim.is_fantasy:
                self.logger.info(f"  å¦„æƒ³æ¡ä»¶: {best_sim.fantasy_condition}")
        
        # ========================================
        # æ­¥éª¤3: æ€è€ƒ â†’ è¡ŒåŠ¨
        # ========================================
        self.logger.info("\næ­¥éª¤3: æ€è€ƒä¸è¡ŒåŠ¨")
        
        # æ€è€ƒï¼ˆå¯èƒ½éœ€è¦å¤šæ¬¡APIè°ƒç”¨ï¼‰
        thought_count_before = self.acting_bot.thought_gen.thought_count
        thought, action = self.acting_bot.think_and_act(
            context=user_input,
            current_desires=desires_before
        )
        thought_count_this_cycle = self.acting_bot.thought_gen.thought_count - thought_count_before
        self.total_thought_count += thought_count_this_cycle
        
        self.logger.info(f"  æ€è€ƒæ¬¡æ•°: {thought_count_this_cycle}")
        
        # è¾“å‡ºå®Œæ•´æ€è€ƒå†…å®¹
        full_thought_content = thought.get('content', '')
        if full_thought_content:
            self.logger.info(f"\n  ã€å®Œæ•´æ€è€ƒè¿‡ç¨‹ã€‘\n{self._format_thought_content(full_thought_content)}")
        
        self.logger.info(f"\n  å†³ç­–: {thought['decision'].get('chosen_action')}")
        self.logger.info(f"  è¡ŒåŠ¨: {action[:100]}...")
        
        # ========================================
        # æ­¥éª¤4: åŸºäºæ€è€ƒæ›´æ–°æ¬²æœ›ï¼ˆé€»è¾‘é—­ç¯æ—¶ï¼‰
        # ========================================
        if thought.get('logical_closure', False):
            self.logger.info("\næ­¥éª¤4: åŸºäºæ€è€ƒæ›´æ–°æ¬²æœ›ï¼ˆè¾¾åˆ°é€»è¾‘é—­ç¯ï¼‰")
            thought_delta = self.desire_updater.update_from_thought(thought, desires_before)
            self.desire_updater.apply_update(thought_delta)
            
            self.logger_mgr.desire_logger.log_change(
                cycle_id=self.cycle_count,
                before=desires_before,
                after=self.desire_manager.get_current_desires(),
                trigger='thought',
                context={'logical_closure': True}
            )
        
        # ========================================
        # æ­¥éª¤5: å‹ç¼©æ€è€ƒ
        # ========================================
        self.logger.info("\næ­¥éª¤5: å‹ç¼©æ€è€ƒ")
        compressed = self.compressor.compress(
            full_thought=thought.get('content', ''),
            context=user_input,
            action=action,
            decision=thought.get('decision')
        )
        self.logger.info(f"  æ‘˜è¦: {compressed['summary']}")
        
        # ========================================
        # æ„å»ºç»“æœ
        # ========================================
        result = {
            'cycle_id': self.cycle_count,
            'timestamp': cycle_start_time,
            'user_input': user_input,
            'desires_before': desires_before,
            'desires_after': self.desire_manager.get_current_desires(),
            'purpose': purpose,
            'purpose_desires': purpose_desires,
            'means_type': means_type,
            'means_desc': means_desc,
            'means_bias': means_bias,
            'means_simulations': all_simulations,  # æ‰€æœ‰æ‰‹æ®µæ¨¡æ‹Ÿç»“æœ
            'thought': thought,
            'thought_count': thought_count_this_cycle,
            'action': action,
            'compressed_thought': compressed,
            'elapsed_time': time.time() - cycle_start_time
        }
        
        # è®°å½•å‘¨æœŸæ—¥å¿—
        self.logger_mgr.cycle_logger.log_cycle(
            cycle_id=self.cycle_count,
            user_input=user_input,
            thought_summary=compressed['summary'],
            action=action,
            response=None,  # æš‚æ—¶æ²¡æœ‰å“åº”
            desires_before=desires_before,
            desires_after=result['desires_after'],
            thought_count=thought_count_this_cycle
        )
        
        self.logger.info(f"\nå†³ç­–å‘¨æœŸ #{self.cycle_count} å®Œæˆï¼Œè€—æ—¶ {result['elapsed_time']:.2f}ç§’")
        
        return result
    
    def handle_response(self,
                       cycle_result: Dict[str, Any],
                       response: str,
                       response_type: str = 'neutral') -> Experience:
        """
        å¤„ç†ç¯å¢ƒå“åº”å¹¶è®°å½•ç»éªŒ
        
        Args:
            cycle_result: ä¹‹å‰run_cycleçš„è¿”å›ç»“æœ
            response: ç¯å¢ƒå“åº”ï¼ˆä¾‹å¦‚ç”¨æˆ·çš„ä¸‹ä¸€æ¡æ¶ˆæ¯ï¼‰
            response_type: å“åº”ç±»å‹ ('positive', 'negative', 'neutral')
        
        Returns:
            è®°å½•çš„ç»éªŒå¯¹è±¡
        """
        self.logger.info(f"\nå¤„ç†ç¯å¢ƒå“åº”: {response[:100]}...")
        
        # ========================================
        # æ­¥éª¤6: è§£æå“åº”ä¿¡å·
        # ========================================
        response_dict = {
            'text': response,
            'signals': self._estimate_response_signals(response, response_type)
        }
        
        # ========================================
        # æ­¥éª¤7: åŸºäºå“åº”æ›´æ–°æ¬²æœ›
        # ========================================
        desires_before_response = self.desire_manager.get_current_desires()
        response_delta = self.desire_updater.update_from_response(
            response_dict,
            desires_before_response
        )
        self.desire_updater.apply_update(response_delta)
        desires_after_response = self.desire_manager.get_current_desires()
        
        # è®¡ç®—æ€»å¹¸ç¦åº¦å˜åŒ–
        total_happiness_delta = sum(response_delta.values())
        
        self.logger_mgr.desire_logger.log_change(
            cycle_id=cycle_result['cycle_id'],
            before=desires_before_response,
            after=desires_after_response,
            trigger='response',
            context={'response_type': response_type}
        )
        
        # ========================================
        # æ­¥éª¤8: åº”ç”¨è¾¹é™…æ•ˆç”¨ï¼ˆåŸºäºå¯è¾¾æˆæ€§ï¼‰
        # ========================================
        # è®¡ç®—å„ç›®çš„çš„å¯è¾¾æˆæ€§
        purpose_achievability = self._calculate_purpose_achievability(cycle_result)
        
        # åº”ç”¨è¾¹é™…æ•ˆç”¨é€’å‡
        desires_after_owning = self.bias_system.apply_owning_bias_with_achievability(
            current_desires=desires_after_response,
            purpose_achievability=purpose_achievability
        )
        
        # æ›´æ–°æ¬²æœ›ç®¡ç†å™¨
        owning_delta = {
            k: desires_after_owning[k] - desires_after_response[k]
            for k in desires_after_response
        }
        self.desire_updater.apply_update(owning_delta)
        
        # ========================================
        # æ­¥éª¤9: è®°å½•ç»éªŒ
        # ========================================
        # è®¡ç®—æˆå°±æ„Ÿ
        achievement_multiplier = 1.0
        is_achievement = False
        
        if response_type == 'positive' and cycle_result['thought_count'] >= 2:
            achievement_multiplier = self._calculate_achievement_multiplier(
                thought_count=cycle_result['thought_count'],
                success=True
            )
            is_achievement = True
        
        # åˆ›å»ºç»éªŒ
        exp = Experience(
            id=0,  # ä¼šç”±æ•°æ®åº“åˆ†é…
            timestamp=cycle_result['timestamp'],
            cycle_id=cycle_result['cycle_id'],
            context=cycle_result['user_input'],
            context_hash=Experience.create_context_hash(cycle_result['user_input']),
            purpose=cycle_result['purpose'],
            purpose_desires=cycle_result['purpose_desires'],
            means=cycle_result['means_desc'],
            means_type=cycle_result['means_type'],
            thought_summary=cycle_result['compressed_thought']['summary'],
            thought_key_elements=cycle_result['compressed_thought']['key_elements'],
            causal_link=cycle_result['compressed_thought']['causal_link'],
            thought_count=cycle_result['thought_count'],
            logical_closure=cycle_result['thought'].get('logical_closure', False),
            action_text=cycle_result['action'],
            response=response,
            response_type=response_type,
            desire_delta=response_delta,
            total_happiness_delta=total_happiness_delta,
            means_effectiveness=max(0.0, min(1.0, (total_happiness_delta + 1) / 2)),  # æ˜ å°„åˆ°0-1
            purpose_achieved=(response_type == 'positive'),
            purpose_achievement_degree=max(0.0, total_happiness_delta) if response_type == 'positive' else 0.0,
            achievement_multiplier=achievement_multiplier,
            is_achievement=is_achievement
        )
        
        # æ’å…¥æ•°æ®åº“
        exp_id = self.memory.insert_experience(exp)
        exp.id = exp_id
        
        # æ›´æ–°ç›®çš„è®°å½•
        self.memory.update_purpose_record(
            purpose=cycle_result['purpose'],
            means=cycle_result['means_type'],
            effectiveness=exp.means_effectiveness,
            success=(response_type == 'positive')
        )
        
        # è®°å½•åˆ°é•¿è®°å¿†
        dominant_desire = max(cycle_result['desires_before'], 
                             key=cycle_result['desires_before'].get)
        
        self.long_memory.add_memory(
            cycle_id=cycle_result['cycle_id'],
            situation=cycle_result['user_input'][:100],
            action_taken=cycle_result['action'][:100],
            outcome=response_type,
            dominant_desire=dominant_desire,
            happiness_delta=total_happiness_delta,
            tags=[cycle_result['means_type'], 'interaction']
        )
        
        self.logger.info(f"ç»éªŒå·²è®°å½•ï¼ŒID: {exp_id}")
        self.logger.info(f"é•¿è®°å¿†å·²æ›´æ–°ï¼Œå…± {len(self.long_memory)} æ¡è®°å¿†")
        
        return exp
    
    def _estimate_response_signals(self, response: str, response_type: str) -> Dict[str, float]:
        """ä¼°è®¡å“åº”çš„ä¿¡å·å¼ºåº¦"""
        signals = {
            'threat': 0.0,
            'misunderstanding': 0.0,
            'uncertainty': 0.0,
            'control_opportunity': 0.0,
            'recognition': 0.0
        }
        
        if response_type == 'positive':
            signals['recognition'] = 0.8
            signals['uncertainty'] = 0.2
        elif response_type == 'negative':
            signals['threat'] = 0.6
            signals['misunderstanding'] = 0.5
        else:
            signals['uncertainty'] = 0.5
        
        # åŸºäºå…³é”®è¯å¾®è°ƒ
        if 'ä¸å¯¹' in response or 'é”™' in response:
            signals['misunderstanding'] += 0.3
        if 'å¾ˆå¥½' in response or 'ä¸é”™' in response or 'èµ' in response:
            signals['recognition'] += 0.2
        
        # é™åˆ¶åœ¨0-1èŒƒå›´
        return {k: min(1.0, max(0.0, v)) for k, v in signals.items()}
    
    def _calculate_purpose_achievability(self, cycle_result: Dict[str, Any]) -> Dict[str, float]:
        """è®¡ç®—å„æ¬²æœ›å¯¹åº”ç›®çš„çš„å¯è¾¾æˆæ€§"""
        # ç®€åŒ–å®ç°ï¼šæ ¹æ®æ‰‹æ®µbiaså’Œå†å²æˆåŠŸç‡ä¼°ç®—
        achievability = {}
        
        for desire_name in cycle_result['desires_before'].keys():
            # æŸ¥è¯¢ç›¸å…³ç»éªŒ
            related_exps = self.memory.query_by_desire(desire_name, threshold=0.1)
            
            if related_exps:
                success_rate = sum(1 for exp in related_exps if exp.purpose_achieved) / len(related_exps)
                achievability[desire_name] = success_rate
            else:
                achievability[desire_name] = 0.5  # é»˜è®¤ä¸­æ€§
        
        return achievability
    
    def _calculate_achievement_multiplier(self, thought_count: int, success: bool) -> float:
        """è®¡ç®—æˆå°±æ„ŸåŠ æˆ"""
        if not success:
            return 1.0
        
        base = self.config.memory.achievement_base_multiplier
        weight = self.config.memory.achievement_thought_weight
        max_mult = self.config.memory.max_achievement_multiplier
        
        multiplier = base + (thought_count - 1) * weight
        return min(multiplier, max_mult)
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯"""
        return {
            'system': {
                'cycle_count': self.cycle_count,
                'total_thought_count': self.total_thought_count
            },
            'desires': self.desire_manager.get_current_desires(),
            'memory': self.memory.get_statistics(),
            'long_memory': self.long_memory.get_statistics(),
            'retrieval': self.retriever.get_retrieval_stats(),
            'scenario': {
                'current_situation': self.scenario_simulator.current_scenario.current_situation,
                'role': self.scenario_simulator.current_scenario.role,
                'predicted_existing': self.scenario_simulator.current_scenario.predicted_existing,
                'predicted_power': self.scenario_simulator.current_scenario.predicted_power,
                'predicted_understanding': self.scenario_simulator.current_scenario.predicted_understanding,
                'predicted_information': self.scenario_simulator.current_scenario.predicted_information,
                'simulations_count': len(self.scenario_simulator.simulation_history)
            }
        }
    
    def __repr__(self) -> str:
        return f"FakeManSystem(cycles={self.cycle_count}, memories={len(self.memory)})"


if __name__ == '__main__':
    # æŒç»­æ€è€ƒæ¨¡å¼
    from dotenv import load_dotenv
    import os
    
    load_dotenv()
    
    if not os.getenv('DEEPSEEK_API_KEY'):
        print("é”™è¯¯: è¯·è®¾ç½® DEEPSEEK_API_KEY ç¯å¢ƒå˜é‡")
        print("åœ¨ .env æ–‡ä»¶ä¸­æ·»åŠ : DEEPSEEK_API_KEY=your_key_here")
        exit(1)
    
    # åˆ›å»ºé…ç½®
    config = Config()
    
    print("="*60)
    print("FakeMan æŒç»­æ€è€ƒç³»ç»Ÿ (AIè‡ªä¸»å†³ç­–ç‰ˆ)")
    print("="*60)
    print("\næ­£åœ¨åˆå§‹åŒ–...")
    
    # åˆ›å»ºç³»ç»Ÿ
    system = FakeManSystem(config)
    
    print(f"\n{system}")
    print(f"\nå½“å‰æ¬²æœ›çŠ¶æ€:")
    for desire, value in system.desire_manager.get_current_desires().items():
        print(f"  {desire:15s}: {value:.3f}")
    
    print(f"\né€šä¿¡æ–‡ä»¶ç›®å½•: {system.comm.comm_dir}")
    print(f"  - è¾“å…¥æ–‡ä»¶: {system.comm.input_file.name}")
    print(f"  - è¾“å‡ºæ–‡ä»¶: {system.comm.output_file.name}")
    print(f"  - çŠ¶æ€æ–‡ä»¶: {system.comm.state_file.name}")
    
    print("\nâœ¨ æ–°ç‰¹æ€§:")
    print("  â€¢ æ¯ç§’è¿›è¡ŒLLMæ·±åº¦æ€è€ƒ")
    print("  â€¢ AIå®Œå…¨è‡ªä¸»å†³ç­–æ˜¯å¦ä¸»åŠ¨è¡ŒåŠ¨")
    print("  â€¢ æ— ç¡¬ç¼–ç é˜ˆå€¼ï¼Œçº¯AIåˆ¤æ–­")
    print("  â€¢ åœºæ™¯æ¨¡æ‹Ÿ + å¦„æƒ³ç”Ÿæˆ + é•¿è®°å¿†")
    
    print("\næç¤º:")
    print("  - ç³»ç»Ÿå°†æŒç»­è¿è¡Œï¼Œæ¯ç§’LLMæ€è€ƒä¸€æ¬¡")
    print("  - ä½¿ç”¨ chat.py è¿›è¡Œäº¤äº’")
    print("  - æŒ‰ Ctrl+C åœæ­¢ç³»ç»Ÿ")
    print("\nå¯åŠ¨æŒç»­æ€è€ƒå¾ªç¯...")
    print("="*60)
    
    # å¯åŠ¨æŒç»­æ€è€ƒå¾ªç¯
    try:
        system.continuous_thinking_loop()
    except Exception as e:
        print(f"\nç³»ç»Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        system.comm.write_state({'status': 'error', 'error': str(e)})

